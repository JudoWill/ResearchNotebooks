{
 "metadata": {
  "name": "ReTreeingFuncs"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import DataFrame, Series, merge, read_csv, MultiIndex, Index, concat\n",
      "from subprocess import check_call\n",
      "from tempfile import NamedTemporaryFile as NTF\n",
      "import os, os.path\n",
      "import numpy as np\n",
      "from scipy.stats import ttest_ind\n",
      "from itertools import groupby,combinations, islice\n",
      "from operator import itemgetter\n",
      "from Bio import Phylo\n",
      "import networkx\n",
      "import sys\n",
      "import pickle\n",
      "\n",
      "from random import shuffle\n",
      "import csv, shlex, shutil\n",
      "\n",
      "os.chdir('/home/will/HIVTropism//')\n",
      "sys.path.append('/home/will/HIVReportGen/AnalysisCode/')\n",
      "sys.path.append('/home/will/PySeqUtils/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from SeqProcessTools import read_pat_seq_data, load_training_seq_data, align_seq_data_frame\n",
      "from GeneralSeqTools import fasta_reader, WebPSSM_V3_fasta, yield_chunks\n",
      "from Bio.Seq import Seq\n",
      "from Bio.Alphabet import generic_dna\n",
      "import glob\n",
      "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
      "from itertools import chain, product"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/dist-packages/futures/__init__.py:24: DeprecationWarning: The futures package has been deprecated. Use the concurrent.futures package instead.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('trop_dict.pkl') as handle:\n",
      "    trop_dict = pickle.load(handle)\n",
      "\n",
      "with open('wanted_data.pkl') as handle:\n",
      "    wanted_data = pickle.load(handle)\n",
      "\n",
      "trans_dict = wanted_data['Name'].to_dict()\n",
      "ntrop_dict = dict((trans_dict[key], val) for key, val in trop_dict.items())\n",
      "trop_dict = ntrop_dict\n",
      "\n",
      "wanted_data = wanted_data.set_index('Name')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from GeneralSeqTools import fasta_writer\n",
      "fourkb_cols = ['gp120-seq-align', 'Nef-seq-align', 'Vpr-seq-align', \n",
      "                 'Tat-1-seq-align', 'Tat-2-seq-align', 'LTR-seq-align']\n",
      "four = wanted_data[fourkb_cols].dropna()\n",
      "wseqs = set()\n",
      "with open('/home/will/Dropbox/HIVseqs/BensTropismLabels.csv') as handle:\n",
      "    for row in csv.DictReader(handle, delimiter=','):\n",
      "        wseqs.add(row['Patient ID'])\n",
      "\n",
      "        \n",
      "for col in four.columns:\n",
      "    found = set()\n",
      "    prot = col.rsplit('-', 2)[0]\n",
      "    fname = 'AlignForBenj/fourKB_%s.fasta' % prot\n",
      "    with open(fname, 'w') as handle:\n",
      "        for seq, name in zip(four[col], four.index):\n",
      "            if name in wseqs and name not in found:\n",
      "                fasta_writer(handle, [(name+'-'+trop_dict[name], ''.join(seq))])\n",
      "                found.add(name)\n",
      "    print prot, len(found)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "gp120 25\n",
        "Nef 25\n",
        "Vpr 25\n",
        "Tat-1 25\n",
        "Tat-2 25\n",
        "LTR 25\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wseqs = set(wanted_data['gp120'].dropna().index)\n",
      "cols = ['gp120-seq-align', 'Nef-seq-align', 'Vpr-seq-align', \n",
      "                 'Tat-1-seq-align', 'Tat-2-seq-align', 'LTR-seq-align']\n",
      "for col in cols:\n",
      "    found = set()\n",
      "    prot = col.rsplit('-', 2)[0]\n",
      "    fname = 'AlignForBenj/has_env_%s.fasta' % prot\n",
      "    df = wanted_data[col].dropna()\n",
      "    with open(fname, 'w') as handle:\n",
      "        for seq, name in zip(df, df.index):\n",
      "            if name in wseqs and name not in found:\n",
      "                fasta_writer(handle, [(name+'-'+trop_dict[name], ''.join(seq))])\n",
      "                found.add(name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def yield_regions(trop_dict):\n",
      "    \n",
      "    regions = ['LTR-seq-align',\n",
      "               'gp41-seq-align',\n",
      "               'gp120-seq-align',\n",
      "               'Nef-seq-align',\n",
      "               'Vpr-seq-align',\n",
      "               'Tat-1-seq-align',\n",
      "               'Tat-2-seq-align',\n",
      "                ]\n",
      "    tail_cols = ['gp120', 'gp41', 'Nef', 'Vpr', \n",
      "                 'Tat-1', 'Tat-2', 'LTR']\n",
      "    fourkb_cols = ['gp120', 'Nef', 'Vpr', \n",
      "                 'Tat-1', 'Tat-2', 'LTR']\n",
      "    groups = [('fourkb', wanted_data[fourkb_cols].dropna().index),\n",
      "              ('full_env', wanted_data[['gp120', 'gp41']].dropna().index),\n",
      "              ('full_tail', wanted_data[tail_cols].dropna().index),\n",
      "              ]\n",
      "    subs = ['SubB']\n",
      "    win_sizes = [5, 10, 15, 20, 30, 35]\n",
      "    \n",
      "    for region, (gname, ind), sub in product(regions, groups, subs):\n",
      "        prot = region.split('-')[0]\n",
      "        gwanted = wanted_data.ix[ind]\n",
      "        mask = gwanted['Sub'] == sub\n",
      "        seq_ser = gwanted[mask][region].dropna()\n",
      "        print prot, gname, sub, len(seq_ser)\n",
      "        seqs = [(name, ''.join(list(seq))) for name, seq in zip(seq_ser.index, seq_ser.values)]\n",
      "        seq_len = len(seqs[0][1])\n",
      "        \n",
      "        for win, start in product(win_sizes, range(seq_len)):\n",
      "            stop = start+win\n",
      "            if stop < seq_len:\n",
      "                nseqs = [(name, seq[start:stop]) for name, seq in seqs]\n",
      "                yield gname, sub, prot, start, win, nseqs, trop_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import dendropy\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from Bio.Alphabet import generic_dna, generic_protein\n",
      "import TreeingTools\n",
      "def calculate_region(arg):\n",
      "    gname, sub, prot, start, win, nseqs, trop_dict = arg\n",
      "    \n",
      "    treename = 'newphyliptrees/%s-%s-%s-%i-%i.tree' % (gname, sub, prot, start, win)\n",
      "    matfname = 'newphyliptrees/%s-%s-%s-%i-%i.pkl' % (gname, sub, prot, start, win)\n",
      "    \n",
      "    if os.path.exists(treename):\n",
      "        #benj_res = 'Already Processed'\n",
      "        #return gname, sub, prot, win, start, benj_res\n",
      "        \n",
      "        with open(matfname) as handle:\n",
      "            dmat = pickle.load(handle)\n",
      "            \n",
      "        with open(treename) as handle:\n",
      "            tree = dendropy.Tree.get_from_stream(handle, 'newick')\n",
      "        \n",
      "    else:\n",
      "        \n",
      "        is_aa = prot != 'LTR'\n",
      "        alphabet = generic_protein if is_aa else generic_dna\n",
      "        \n",
      "        try:\n",
      "            tree, dmat = TreeingTools.phylip_tree_collapse_unique(nseqs, alphabet=alphabet)\n",
      "        except ValueError:\n",
      "            benj_res = 'Too few unique sequences to process'\n",
      "            return gname, sub, prot, win, start, benj_res\n",
      "        except:\n",
      "            benj_res = 'uncaught exception in dist-mat'\n",
      "            return gname, sub, prot, win, start, benj_res\n",
      "        with open(matfname, 'w') as handle:\n",
      "            pickle.dump(dmat, handle)\n",
      "        with open(treename, 'w') as handle:\n",
      "            tree.write_to_stream(handle, 'newick')\n",
      "    \n",
      "    try:\n",
      "        benj_res = TreeingTools.check_distance_pvals(dmat, trop_dict, nreps = 50)\n",
      "    except AssertionError:\n",
      "        benj_res = 'too few groups'\n",
      "        return  gname, sub, prot, win, start, benj_res\n",
      "    except:\n",
      "        benj_res = 'uncaught exception'\n",
      "        return  gname, sub, prot, win, start, benj_res\n",
      "    \n",
      "    \n",
      "    try:\n",
      "        out = TreeingTools.evaluate_association_index(tree, trop_dict)\n",
      "        benj_res['AI'], benj_res['AI-pval'], benj_res['AI-null'] = out\n",
      "    except:\n",
      "        benj_res['AI'], benj_res['AI-pval'], benj_res['AI-null'] = ('error', 'error', 'error')\n",
      "    \n",
      "    return gname, sub, prot, win, start, benj_res\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import groupby, imap\n",
      "from operator import itemgetter\n",
      "from types import StringType\n",
      "from concurrent.futures import ThreadPoolExecutor\n",
      "\n",
      "\n",
      "benj_fields = ['GroupName',\n",
      "               'Subtype',\n",
      "               'Prot',\n",
      "                'Start',\n",
      "                'WinSize',\n",
      "                'Group2Mean',\n",
      "                'Group2Std',\n",
      "                'Group2Name',\n",
      "                'Group1Mean',\n",
      "                'Group1Std',\n",
      "                'RawPval',\n",
      "                'AdjPval',\n",
      "                'Group1Name',\n",
      "                'AI',\n",
      "                'AI-pval',\n",
      "                'AI-null']\n",
      "fname = 'more_phylip_BenjRes.tsv'\n",
      "benj_writer = csv.DictWriter(open(fname, 'w'), benj_fields, delimiter = '\\t')\n",
      "   \n",
      "\n",
      "benj_writer.writeheader()\n",
      "\n",
      "multi = True\n",
      "print 'Starting multiprocessing!'\n",
      "if multi:\n",
      "    pool = ProcessPoolExecutor(max_workers = 30)\n",
      "    results = pool.map(calculate_region, yield_regions(trop_dict))\n",
      "else:\n",
      "    results = imap(calculate_region, islice(yield_regions(trop_dict), 0,35))\n",
      "\n",
      "for gname, sub, prot, win, start, benj_res in results:\n",
      "    \n",
      "    #print prot, start, win\n",
      "    tdict = {\n",
      "             'Prot':prot,\n",
      "             'Start':start,\n",
      "             'WinSize':win,\n",
      "             'GroupName':gname,\n",
      "             'Subtype':sub,\n",
      "             }\n",
      "    if type(benj_res) is StringType:\n",
      "        if (benj_res == 'Already Processed') or benj_res.startswith('Too few unique sequences'):\n",
      "            continue\n",
      "        print benj_res, prot, start, win\n",
      "    else:\n",
      "        benj_res.update(tdict)\n",
      "        benj_writer.writerow(benj_res)\n",
      "    \n",
      "        \n",
      "if multi:\n",
      "    pool.shutdown()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting multiprocessing!\n",
        "LTR"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " fourkb SubB 304\n",
        "gp41"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " fourkb SubB 304\n",
        "gp120"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " fourkb SubB 304\n",
        "Nef"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " fourkb SubB 304\n",
        "Vpr"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " fourkb SubB 304\n",
        "Tat fourkb SubB 304\n",
        "Tat"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " fourkb SubB 304\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "uncaught exception gp41 156 5\n",
        "uncaught exception gp41 157 5\n",
        "uncaught exception gp41 158 5\n",
        "uncaught exception"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " gp41 302 5\n",
        "uncaught exception gp41 303 5\n",
        "uncaught exception gp41 304 5\n",
        "uncaught exception gp41 305 5\n",
        "uncaught exception gp41 306 5\n",
        "uncaught exception gp41 307 5\n",
        "uncaught exception"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " gp41 302 10\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}